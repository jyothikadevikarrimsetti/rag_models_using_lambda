{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "302ebb34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in c:\\users\\jimmy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.11.7)\n",
      "Requirement already satisfied: pdfminer.six==20250506 in c:\\users\\jimmy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pdfplumber) (20250506)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\jimmy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pdfplumber) (11.3.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\jimmy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pdfplumber) (4.30.0)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\jimmy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (3.4.3)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\jimmy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pdfminer.six==20250506->pdfplumber) (39.0.2)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\jimmy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.0.0)\n",
      "Requirement already satisfied: pycparser in c:\\users\\jimmy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20250506->pdfplumber) (2.23)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\jimmy\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f63661db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in c:\\users\\jimmy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.15.1)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in c:\\users\\jimmy\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pymongo) (2.8.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.2\n",
      "[notice] To update, run: C:\\Users\\jimmy\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c2bfdec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1140 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Agreement Metadata =====\n",
      "Metadata: {}\n",
      "AGREEMENT\n",
      "GOVERNMENT OF ANDHRA ...\n",
      "\n",
      "\n",
      "===== Scope of Work =====\n",
      "Metadata: {}\n",
      "scope of works. The Bidder sha ...\n",
      "\n",
      "\n",
      "===== Staffing/Personnel Requirements =====\n",
      "Metadata: {'role': 'Project Manager'}\n",
      "Item No. | Position/specializa ...\n",
      "\n",
      "\n",
      "===== Staffing/Personnel Requirements =====\n",
      "Metadata: {'role': 'Transportation Manager'}\n",
      "Item No. | Position/specializa ...\n",
      "\n",
      "\n",
      "===== Staffing/Personnel Requirements =====\n",
      "Metadata: {'role': 'Structural Manager'}\n",
      "Item No. | Position/specializa ...\n",
      "\n",
      "\n",
      "===== Staffing/Personnel Requirements =====\n",
      "Metadata: {'role': 'Manager – QS & Billing'}\n",
      "Item No. | Position/specializa ...\n",
      "\n",
      "\n",
      "===== Staffing/Personnel Requirements =====\n",
      "Metadata: {'role': 'Manager – QA & QC / Materials'}\n",
      "Item No. | Position/specializa ...\n",
      "\n",
      "\n",
      "===== Staffing/Personnel Requirements =====\n",
      "Metadata: {'role': 'Construction Manager'}\n",
      "Item No. | Position/specializa ...\n",
      "\n",
      "\n",
      "===== Staffing/Personnel Requirements =====\n",
      "Metadata: {'role': 'Manager – Construction safety'}\n",
      "Item No. | Position/specializa ...\n",
      "\n",
      "\n",
      "===== Staffing/Personnel Requirements =====\n",
      "Metadata: {'role': 'Manager – Environment & Social Development'}\n",
      "Item No. | Position/specializa ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "from transformers import GPT2TokenizerFast\n",
    " \n",
    "# Load tokenizer to count tokens\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    " \n",
    "# Load PDF\n",
    "\n",
    "doc_path = r\"C:\\Users\\jimmy\\Downloads\\N1_Trunk_Infra_Agreement1.pdf\"\n",
    "\n",
    "reader = PdfReader(doc_path)\n",
    " \n",
    "# Extract all text\n",
    "\n",
    "full_text = \"\"\n",
    "\n",
    "for page in reader.pages:\n",
    "\n",
    "    full_text += page.extract_text() + \"\\n\"\n",
    " \n",
    "# Define section patterns\n",
    "\n",
    "patterns = {\n",
    "\n",
    "    \"Agreement Metadata\": r\"(?is)(agreement.*?value:.*?)(?=Item No.|Scope|Staff|Personnel|Equipment|Notes|Technical)\",\n",
    "\n",
    "    \"Scope of Work\": r\"(?is)(scope of work.*?)(?=Item No.|Staff|Personnel|Equipment|Notes|Technical)\",\n",
    "\n",
    "    \"Staffing/Personnel Requirements\": r\"(?is)(Item No.*?Manager.*?)(?=Technical Requirements|Equipment|Notes|conditions)\",\n",
    "\n",
    "    \"Equipment Requirements\": r\"(?is)(Technical Requirements.*?)(?=Chief Engineer|Notes|conditions)\",\n",
    "\n",
    "    \"Notes and Conditions\": r\"(?is)(Note:.*)\"\n",
    "\n",
    "}\n",
    " \n",
    "# Function to split long sections into chunks\n",
    "\n",
    "def chunk_section(text, section_name, max_tokens=2000, small_chunk=800, overlap=100, metadata=None):\n",
    "\n",
    "    tokens = tokenizer.encode(text)\n",
    "\n",
    "    chunks = []\n",
    " \n",
    "    if len(tokens) <= max_tokens:\n",
    "\n",
    "        chunks.append({\"section\": section_name, \"metadata\": metadata or {}, \"content\": text.strip()})\n",
    "\n",
    "    else:\n",
    "\n",
    "        start = 0\n",
    "\n",
    "        while start < len(tokens):\n",
    "\n",
    "            end = min(start + small_chunk, len(tokens))\n",
    "\n",
    "            chunk_text = tokenizer.decode(tokens[start:end])\n",
    "\n",
    "            chunks.append({\"section\": section_name, \"metadata\": metadata or {}, \"content\": chunk_text.strip()})\n",
    "\n",
    "            start += small_chunk - overlap\n",
    "\n",
    "    return chunks\n",
    " \n",
    "all_chunks = []\n",
    " \n",
    "for name, pattern in patterns.items():\n",
    "    match = re.search(pattern, full_text, re.DOTALL)\n",
    "    if match:\n",
    "        section_text = match.group().strip()\n",
    "\n",
    "        if name == \"Staffing/Personnel Requirements\":\n",
    "            rows = re.findall(r\"\\d+\\.\\s*(.*?)\\s*-.*\", section_text)\n",
    "            for row in rows:\n",
    "                metadata = {\"role\": row.strip()}\n",
    "                section_chunks = chunk_section(section_text, name, metadata=metadata)\n",
    "                all_chunks.extend(section_chunks)\n",
    "\n",
    "        elif name == \"Equipment Requirements\":\n",
    "            items = re.findall(r\"\\d+\\.\\s*([A-Za-z0-9\\- ]+)-\", section_text)\n",
    "            for item in items:\n",
    "                metadata = {\"item\": item.strip()}\n",
    "                section_chunks = chunk_section(section_text, name, metadata=metadata)\n",
    "                all_chunks.extend(section_chunks)\n",
    "\n",
    "        else:\n",
    "            section_chunks = chunk_section(section_text, name)\n",
    "            all_chunks.extend(section_chunks)\n",
    "    else:\n",
    "        all_chunks.append({\"section\": name, \"metadata\": {}, \"content\": \"<Not Found>\"})\n",
    " \n",
    "# Show enriched chunks\n",
    "\n",
    "for c in all_chunks[:10]:  # preview first 10\n",
    "\n",
    "    print(\"\\n=====\", c[\"section\"], \"=====\")\n",
    "\n",
    "    print(\"Metadata:\", c[\"metadata\"])\n",
    "\n",
    "    print(c[\"content\"][:30], \"...\\n\")\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c24c6905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks saved to C:\\Users\\jimmy\\Downloads\\N1_Trunk_Infra_Agreement_chunks.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Save all_chunks to a JSON file\n",
    "output_path = r\"C:\\Users\\jimmy\\Downloads\\N1_Trunk_Infra_Agreement_chunks.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_chunks, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Chunks saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "51eca0ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\jimmy\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (1.108.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\jimmy\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (4.9.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\jimmy\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\jimmy\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\jimmy\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (0.11.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\jimmy\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (2.11.7)\n",
      "Requirement already satisfied: sniffio in c:\\users\\jimmy\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\jimmy\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\jimmy\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from openai) (4.14.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\jimmy\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\jimmy\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2025.6.15)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\jimmy\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\jimmy\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\jimmy\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\jimmy\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\jimmy\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\jimmy\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tiktoken in c:\\users\\jimmy\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.11.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\jimmy\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tiktoken) (2025.9.1)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\jimmy\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from tiktoken) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\jimmy\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jimmy\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jimmy\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jimmy\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2025.6.15)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install openai\n",
    "%pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a315c820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import tiktoken\n",
    "import concurrent.futures\n",
    "import logging\n",
    "\n",
    "load_dotenv(\"../injestion/config/.env\")\n",
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    ")\n",
    "deployment = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "# Helper functions for embeddings and similarity\n",
    "def get_openai_embedding(text, timeout=15):\n",
    "    \"\"\"Get embeddings using Azure OpenAI's text-embedding model with context window truncation and timeout.\"\"\"\n",
    "    # Truncate text to fit within model context window (e.g., 8000 tokens for text-embedding-3-small)\n",
    "    max_tokens = 8000\n",
    "    encoding = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
    "    tokens = encoding.encode(text)\n",
    "    if len(tokens) > max_tokens:\n",
    "        tokens = tokens[:max_tokens]\n",
    "        text = encoding.decode(tokens)\n",
    "    def call():\n",
    "        return client.embeddings.create(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            input=text\n",
    "        ).data[0].embedding\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:\n",
    "        future = executor.submit(call)\n",
    "        try:\n",
    "            return future.result(timeout=timeout)\n",
    "        except concurrent.futures.TimeoutError:\n",
    "            logging.error(f\"OpenAI embedding call timed out for text: {text[:50]}\")\n",
    "            raise TimeoutError(\"OpenAI embedding call timed out.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c455429a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "95f9968a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in c:\\users\\jimmy\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (4.15.1)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in c:\\users\\jimmy\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from pymongo) (2.8.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "c55ba47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Inserted module: 68cbf1e48bc9122af55fc36f\n",
      "✅ Module created with ID: 68cbf1e48bc9122af55fc36f\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jimmy\\AppData\\Local\\Temp\\ipykernel_27116\\2720541164.py:37: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  modified_time=datetime.utcnow(),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Upserted knowledge object for module: 68cbf1e48bc9122af55fc36f\n",
      "✅ Knowledge object created successfully\n",
      "🔄 Generating embedding for chunk 1/29...\n",
      "✅ Inserted chunk: 68cbf1e68bc9122af55fc370\n",
      "✅ Inserted chunk 1/29: Agreement Metadata...\n",
      "🔄 Generating embedding for chunk 2/29...\n",
      "✅ Inserted chunk: 68cbf1e68bc9122af55fc371\n",
      "✅ Inserted chunk 2/29: Scope of Work...\n",
      "🔄 Generating embedding for chunk 3/29...\n",
      "✅ Inserted chunk: 68cbf1e78bc9122af55fc372\n",
      "✅ Inserted chunk 3/29: Staffing/Personnel Requirements...\n",
      "🔄 Generating embedding for chunk 4/29...\n",
      "✅ Inserted chunk: 68cbf1e78bc9122af55fc373\n",
      "✅ Inserted chunk 4/29: Staffing/Personnel Requirements...\n",
      "🔄 Generating embedding for chunk 5/29...\n",
      "✅ Inserted chunk: 68cbf1e88bc9122af55fc374\n",
      "✅ Inserted chunk 5/29: Staffing/Personnel Requirements...\n",
      "🔄 Generating embedding for chunk 6/29...\n",
      "✅ Inserted chunk: 68cbf1e88bc9122af55fc375\n",
      "✅ Inserted chunk 6/29: Staffing/Personnel Requirements...\n",
      "🔄 Generating embedding for chunk 7/29...\n",
      "✅ Inserted chunk: 68cbf1e88bc9122af55fc376\n",
      "✅ Inserted chunk 7/29: Staffing/Personnel Requirements...\n",
      "🔄 Generating embedding for chunk 8/29...\n",
      "✅ Inserted chunk: 68cbf1e98bc9122af55fc377\n",
      "✅ Inserted chunk 8/29: Staffing/Personnel Requirements...\n",
      "🔄 Generating embedding for chunk 9/29...\n",
      "✅ Inserted chunk: 68cbf1e98bc9122af55fc378\n",
      "✅ Inserted chunk 9/29: Staffing/Personnel Requirements...\n",
      "🔄 Generating embedding for chunk 10/29...\n",
      "✅ Inserted chunk: 68cbf1ea8bc9122af55fc379\n",
      "✅ Inserted chunk 10/29: Staffing/Personnel Requirements...\n",
      "🔄 Generating embedding for chunk 11/29...\n",
      "✅ Inserted chunk: 68cbf1ea8bc9122af55fc37a\n",
      "✅ Inserted chunk 11/29: Staffing/Personnel Requirements...\n",
      "🔄 Generating embedding for chunk 12/29...\n",
      "✅ Inserted chunk: 68cbf1eb8bc9122af55fc37b\n",
      "✅ Inserted chunk 12/29: Staffing/Personnel Requirements...\n",
      "🔄 Generating embedding for chunk 13/29...\n",
      "✅ Inserted chunk: 68cbf1eb8bc9122af55fc37c\n",
      "✅ Inserted chunk 13/29: Staffing/Personnel Requirements...\n",
      "🔄 Generating embedding for chunk 14/29...\n",
      "✅ Inserted chunk: 68cbf1ec8bc9122af55fc37d\n",
      "✅ Inserted chunk 14/29: Staffing/Personnel Requirements...\n",
      "🔄 Generating embedding for chunk 15/29...\n",
      "✅ Inserted chunk: 68cbf1ec8bc9122af55fc37e\n",
      "✅ Inserted chunk 15/29: Staffing/Personnel Requirements...\n",
      "🔄 Generating embedding for chunk 16/29...\n",
      "✅ Inserted chunk: 68cbf1ed8bc9122af55fc37f\n",
      "✅ Inserted chunk 16/29: Staffing/Personnel Requirements...\n",
      "🔄 Generating embedding for chunk 17/29...\n",
      "✅ Inserted chunk: 68cbf1ed8bc9122af55fc380\n",
      "✅ Inserted chunk 17/29: Equipment Requirements...\n",
      "🔄 Generating embedding for chunk 18/29...\n",
      "✅ Inserted chunk: 68cbf1ee8bc9122af55fc381\n",
      "✅ Inserted chunk 18/29: Equipment Requirements...\n",
      "🔄 Generating embedding for chunk 19/29...\n",
      "✅ Inserted chunk: 68cbf1ee8bc9122af55fc382\n",
      "✅ Inserted chunk 19/29: Equipment Requirements...\n",
      "🔄 Generating embedding for chunk 20/29...\n",
      "✅ Inserted chunk: 68cbf1ef8bc9122af55fc383\n",
      "✅ Inserted chunk 20/29: Equipment Requirements...\n",
      "🔄 Generating embedding for chunk 21/29...\n",
      "✅ Inserted chunk: 68cbf1ef8bc9122af55fc384\n",
      "✅ Inserted chunk 21/29: Equipment Requirements...\n",
      "🔄 Generating embedding for chunk 22/29...\n",
      "✅ Inserted chunk: 68cbf1f08bc9122af55fc385\n",
      "✅ Inserted chunk 22/29: Equipment Requirements...\n",
      "🔄 Generating embedding for chunk 23/29...\n",
      "✅ Inserted chunk: 68cbf1f08bc9122af55fc386\n",
      "✅ Inserted chunk 23/29: Equipment Requirements...\n",
      "🔄 Generating embedding for chunk 24/29...\n",
      "✅ Inserted chunk: 68cbf1f18bc9122af55fc387\n",
      "✅ Inserted chunk 24/29: Equipment Requirements...\n",
      "🔄 Generating embedding for chunk 25/29...\n",
      "✅ Inserted chunk: 68cbf1f18bc9122af55fc388\n",
      "✅ Inserted chunk 25/29: Equipment Requirements...\n",
      "🔄 Generating embedding for chunk 26/29...\n",
      "✅ Inserted chunk: 68cbf1f28bc9122af55fc389\n",
      "✅ Inserted chunk 26/29: Equipment Requirements...\n",
      "🔄 Generating embedding for chunk 27/29...\n",
      "✅ Inserted chunk: 68cbf1f28bc9122af55fc38a\n",
      "✅ Inserted chunk 27/29: Equipment Requirements...\n",
      "🔄 Generating embedding for chunk 28/29...\n",
      "✅ Inserted chunk: 68cbf1f38bc9122af55fc38b\n",
      "✅ Inserted chunk 28/29: Equipment Requirements...\n",
      "🔄 Generating embedding for chunk 29/29...\n",
      "✅ Inserted chunk: 68cbf1f38bc9122af55fc38c\n",
      "✅ Inserted chunk 29/29: Notes and Conditions...\n",
      "\n",
      "🎉 Successfully processed 29 out of 29 chunks\n",
      "✅ Updated knowledge object with 29 chunk IDs\n",
      "✅ Updated knowledge object with chunk IDs\n",
      "\n",
      "📊 Summary:\n",
      "  - Module: 68cbf1e48bc9122af55fc36f\n",
      "  - Knowledge Object: N1 Trunk Infrastructure Agreement\n",
      "  - Chunks processed: 29\n",
      "  - MongoDB collections updated: modules, knowledge_objects, chunks\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Add the injestion directory to Python path\n",
    "sys.path.append(os.path.abspath(\"../injestion\"))\n",
    "\n",
    "from models.datamodel_pdantic import Module, KnowledgeObject, Chunk, EmbeddingMeta, Metadata\n",
    "from scripts.mongo_utils import (\n",
    "    insert_module, \n",
    "    insert_knowledge_object, \n",
    "    insert_chunk, \n",
    "    create_embedding_meta\n",
    ")\n",
    "\n",
    "# 1. Create and insert Module\n",
    "module_data = {\n",
    "    \"module_id\": \"N1_Trunk_Infra_Agreement\",\n",
    "    \"module_tag\": [\"infrastructure\", \"construction\", \"amaravati\", \"roads\"],\n",
    "    \"module_link\": [\"https://example.com/n1-trunk-agreement\"]\n",
    "}\n",
    "\n",
    "try:\n",
    "    module_obj = Module(**module_data)\n",
    "    module_id = insert_module(module_obj)  # This returns the MongoDB ObjectId as string\n",
    "    print(f\"✅ Module created with ID: {module_id}\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Module creation failed: {e}\")\n",
    "    module_id = \"N1_Trunk_Infra_Agreement\"  # fallback\n",
    "\n",
    "# 2. Create Metadata for Knowledge Object\n",
    "metadata = Metadata(\n",
    "    path=\"/documents/N1_Trunk_Infra_Agreement.pdf\",\n",
    "    repo_url=\"https://github.com/your-repo/documents\",\n",
    "    intent_category=\"construction_agreement\",\n",
    "    version=\"1.0\",\n",
    "    modified_time=datetime.utcnow(),\n",
    "    csp=\"amaravati_development\"\n",
    ")\n",
    "\n",
    "# 3. Create and insert Knowledge Object\n",
    "knowledge_object_data = {\n",
    "    \"title\": \"N1 Trunk Infrastructure Agreement\",\n",
    "    \"named_entity\": \"BSR Infratech India Limited\",\n",
    "    \"summary\": \"Construction agreement for smart trunk infrastructure in Amaravati Capital City including roads, storm water drains, water supply network, sewerage network, utility ducts, and other infrastructure components.\",\n",
    "    \"content\": \"Agreement for construction of balance smart trunk infrastructure with Roads, Storm Water Drains, Water Supply Network, Sewerage Network, Utility Ducts for Power& ICT, Reuse waterline, Pedestrian tracks, Cycle tracks, Avenue Plantation& Street Furniture etc. in N1 Road in Amaravati Capital City\",\n",
    "    \"keywords\": \"infrastructure, construction, amaravati, roads, drainage, utilities, BSR Infratech\",\n",
    "    \"texts\": \" \".join([chunk[\"content\"] for chunk in all_chunks if chunk[\"content\"] != \"<Not Found>\"]),\n",
    "    \"is_terraform\": False,\n",
    "    \"metadata\": metadata,\n",
    "    \"module_id\": module_id,  # Use the actual ObjectId as string\n",
    "    \"chunk_ids\": []\n",
    "}\n",
    "\n",
    "try:\n",
    "    knowledge_obj = KnowledgeObject(**knowledge_object_data)\n",
    "    insert_knowledge_object(knowledge_obj)\n",
    "    print(\"✅ Knowledge object created successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Knowledge object creation failed: {e}\")\n",
    "\n",
    "# 4. Process and insert Chunks with embeddings\n",
    "chunk_ids = []\n",
    "\n",
    "for i, chunk_dict in enumerate(all_chunks):\n",
    "    try:\n",
    "        # Skip chunks with no content\n",
    "        if chunk_dict[\"content\"] == \"<Not Found>\" or not chunk_dict[\"content\"].strip():\n",
    "            print(f\"⏭️ Skipping empty chunk {i+1}\")\n",
    "            continue\n",
    "            \n",
    "        # Generate embedding for the chunk content\n",
    "        print(f\"🔄 Generating embedding for chunk {i+1}/{len(all_chunks)}...\")\n",
    "        embedding = get_openai_embedding(chunk_dict[\"content\"])\n",
    "        \n",
    "        # Create embedding metadata\n",
    "        embedding_meta = create_embedding_meta(\n",
    "            model_name=\"text-embedding-3-small\",\n",
    "            dimensionality=len(embedding),\n",
    "            embedding_method=\"azure_openai\"\n",
    "        )\n",
    "        \n",
    "        # Prepare chunk data for Pydantic model (matching the expected schema)\n",
    "        chunk_data = {\n",
    "            \"document_id\": module_id,  # Use module_id (ObjectId as string) for document_id\n",
    "            \"chunk_id\": i,\n",
    "            \"chunk_start\": 0,  # You can calculate actual start positions if needed\n",
    "            \"chunk_end\": len(chunk_dict[\"content\"]),\n",
    "            \"chunk_text\": chunk_dict[\"content\"],\n",
    "            \"embedding\": embedding,\n",
    "            \"embedding_meta\": embedding_meta\n",
    "        }\n",
    "        \n",
    "        # Create Chunk object and insert\n",
    "        chunk_obj = Chunk(**chunk_data)\n",
    "        chunk_id = insert_chunk(chunk_obj)\n",
    "        \n",
    "        if chunk_id:\n",
    "            chunk_ids.append(chunk_id)\n",
    "            print(f\"✅ Inserted chunk {i+1}/{len(all_chunks)}: {chunk_dict['section'][:50]}...\")\n",
    "        else:\n",
    "            print(f\"❌ Failed to insert chunk {i+1}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error processing chunk {i+1}: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n🎉 Successfully processed {len(chunk_ids)} out of {len(all_chunks)} chunks\")\n",
    "\n",
    "# 5. Update Knowledge Object with chunk IDs\n",
    "try:\n",
    "    from scripts.mongo_utils import update_knowledge_object_chunk_ids\n",
    "    update_knowledge_object_chunk_ids(\n",
    "        module_id=module_id,\n",
    "        title=knowledge_object_data[\"title\"],\n",
    "        chunk_ids=chunk_ids\n",
    "    )\n",
    "    print(\"✅ Updated knowledge object with chunk IDs\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Failed to update knowledge object with chunk IDs: {e}\")\n",
    "\n",
    "print(f\"\\n📊 Summary:\")\n",
    "print(f\"  - Module: {module_id}\")\n",
    "print(f\"  - Knowledge Object: {knowledge_object_data['title']}\")\n",
    "print(f\"  - Chunks processed: {len(chunk_ids)}\")\n",
    "print(f\"  - MongoDB collections updated: modules, knowledge_objects, chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55647ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
