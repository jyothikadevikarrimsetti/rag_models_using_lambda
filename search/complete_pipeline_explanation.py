"""
Complete explanation of the MongoDB Atlas Vector Search Pipeline.
"""

def explain_complete_pipeline():
    """Explain the entire search pipeline from query to response."""
    
    print("üîç COMPLETE MONGODB ATLAS VECTOR SEARCH PIPELINE EXPLANATION")
    print("=" * 80)
    
    print("\nüåü OVERVIEW:")
    print("Your pipeline transforms a text query into semantically relevant results")
    print("by using vector embeddings and MongoDB Atlas Vector Search capabilities.")
    
    print("\nüìã PIPELINE STAGES BREAKDOWN:")
    print("=" * 50)


def explain_stage_1_vector_search():
    """Explain the $vectorSearch stage."""
    
    print("\nüéØ STAGE 1: $vectorSearch")
    print("-" * 30)
    
    code = '''
    {
        "$vectorSearch": {
            "queryVector": query_vector,           # [1536 float values]
            "path": "embeddings.vector",           # Field containing chunk embeddings
            "numCandidates": 100,                  # Initial search scope
            "index": "vector_index",               # Atlas Vector Search index
            "limit": top_k                         # Final results to return (3)
        }
    }
    '''
    
    print("üìù CODE:")
    print(code)
    
    print("üîç WHAT HAPPENS:")
    print("‚Ä¢ Converts your query 'What is the education background?' to 1536-dimensional vector")
    print("‚Ä¢ Compares this vector with embeddings.vector in ALL chunks using cosine similarity")
    print("‚Ä¢ Finds 100 most similar candidates (numCandidates: 100)")
    print("‚Ä¢ Calculates similarity scores (0.6375, 0.5974, 0.5970 in your case)")
    print("‚Ä¢ Returns top 3 most similar chunks (limit: top_k)")
    print("‚Ä¢ Results are automatically sorted by similarity score (highest first)")
    
    print("\nüìä PERFORMANCE:")
    print("‚Ä¢ Searches through 24 chunks in your database")
    print("‚Ä¢ Uses optimized vector search algorithms")
    print("‚Ä¢ Much faster than comparing embeddings manually")


def explain_stage_2_add_fields():
    """Explain the $addFields stage."""
    
    print("\nüîß STAGE 2: $addFields")
    print("-" * 25)
    
    code = '''
    {
        "$addFields": {
            "document_object_id": {"$toObjectId": "$document_id"}
        }
    }
    '''
    
    print("üìù CODE:")
    print(code)
    
    print("üîç WHAT HAPPENS:")
    print("‚Ä¢ Chunks store document_id as STRING: '68ad8bb8d0540da9d9072fd7'")
    print("‚Ä¢ Documents collection uses ObjectId: ObjectId('68ad8bb8d0540da9d9072fd7')")
    print("‚Ä¢ This stage converts string to ObjectId for proper joining")
    print("‚Ä¢ Creates new field 'document_object_id' without modifying original data")
    print("‚Ä¢ Essential for the next $lookup stages to work correctly")
    
    print("\nüí° WHY NEEDED:")
    print("‚Ä¢ MongoDB $lookup requires matching data types")
    print("‚Ä¢ String ‚â† ObjectId, so lookup would fail without conversion")
    print("‚Ä¢ This is a common pattern when joining across collections")


def explain_stage_3_documents_lookup():
    """Explain the documents lookup stage."""
    
    print("\nüìÅ STAGE 3: $lookup (documents)")
    print("-" * 35)
    
    code = '''
    {
        "$lookup": {
            "from": "documents",                   # Join with documents collection
            "localField": "document_object_id",    # Use converted ObjectId
            "foreignField": "_id",                 # Match with document _id
            "as": "document"                       # Store result as 'document' array
        }
    }
    '''
    
    print("üìù CODE:")
    print(code)
    
    print("üîç WHAT HAPPENS:")
    print("‚Ä¢ Joins each chunk with its parent document")
    print("‚Ä¢ Retrieves document metadata: filename, filepath, creation dates")
    print("‚Ä¢ Result: chunk now has 'document' array with document info")
    print("‚Ä¢ Example: 'Jimson_Ratnam_FullStackDeveloper_2years.pdf'")
    
    print("\nüìä DATA ENRICHMENT:")
    print("Before: chunk only has text and document_id")
    print("After:  chunk has text + document filename + document metadata")


def explain_stage_4_knowledge_lookup():
    """Explain the knowledge objects lookup stage."""
    
    print("\nüß† STAGE 4: $lookup (knowledge_objects)")
    print("-" * 40)
    
    code = '''
    {
        "$lookup": {
            "from": "knowledge_objects",           # Join with knowledge collection
            "localField": "document_object_id",    # Use same ObjectId
            "foreignField": "document_id",         # Match with knowledge.document_id
            "as": "knowledge"                      # Store as 'knowledge' array
        }
    }
    '''
    
    print("üìù CODE:")
    print(code)
    
    print("üîç WHAT HAPPENS:")
    print("‚Ä¢ Joins with document-level AI-generated metadata")
    print("‚Ä¢ Retrieves: summary, keywords, topics, entities")
    print("‚Ä¢ Adds semantic context about the entire document")
    print("‚Ä¢ Example: Document summary, extracted keywords, identified topics")
    
    print("\nüéØ VALUE ADDED:")
    print("‚Ä¢ Chunk-level: specific text content")
    print("‚Ä¢ Document-level: overall context and themes")
    print("‚Ä¢ Combined: rich context for better answers")


def explain_stage_5_project():
    """Explain the $project stage."""
    
    print("\nüì§ STAGE 5: $project")
    print("-" * 20)
    
    code = '''
    {
        "$project": {
            "_id": 1,                              # Chunk ID
            "chunk_text": 1,                       # Actual text content
            "chunk_index": 1,                      # Position in document
            "start_pos": 1,                        # Character start position
            "end_pos": 1,                          # Character end position
            "document.filename": 1,                # Document filename
            "document.filepath": 1,                # Document path
            "knowledge.summary": 1,                # Document summary
            "knowledge.keywords": 1,               # Extracted keywords
            "knowledge.topic": 1,                  # Document topic
            "score": {"$meta": "vectorSearchScore"} # ‚≠ê THE SIMILARITY SCORE
        }
    }
    '''
    
    print("üìù CODE:")
    print(code)
    
    print("üîç WHAT HAPPENS:")
    print("‚Ä¢ Selects which fields to include in final results")
    print("‚Ä¢ Flattens nested arrays (document.filename instead of document[0].filename)")
    print("‚Ä¢ ‚≠ê EXTRACTS THE SIMILARITY SCORE using $meta: 'vectorSearchScore'")
    print("‚Ä¢ Removes unnecessary fields to optimize response size")
    print("‚Ä¢ Structures data for easy consumption by your Python code")
    
    print("\nüéØ KEY INSIGHT:")
    print("‚Ä¢ The score (0.6375, 0.5974, 0.5970) comes from this stage")
    print("‚Ä¢ Without this line, you wouldn't have access to similarity scores")
    print("‚Ä¢ This is what makes the results ranked and meaningful")


def explain_error_handling():
    """Explain the error handling and fallback."""
    
    print("\nüõ°Ô∏è ERROR HANDLING & FALLBACK")
    print("-" * 35)
    
    print("üîç TRY BLOCK:")
    print("‚Ä¢ Attempts vector search with full pipeline")
    print("‚Ä¢ Logs results and scores for debugging")
    print("‚Ä¢ Most common path when everything works")
    
    print("\n‚ö†Ô∏è CATCH BLOCK (Fallback):")
    print("‚Ä¢ Falls back to simple text search if vector search fails")
    print("‚Ä¢ Uses MongoDB regex: {'chunk_text': {'$regex': query_text}}")
    print("‚Ä¢ Still performs document/knowledge lookups manually")
    print("‚Ä¢ Assigns default score of 0.5 to fallback results")
    
    print("\nüéØ WHY FALLBACK:")
    print("‚Ä¢ Vector index might not exist")
    print("‚Ä¢ Network issues with Atlas")
    print("‚Ä¢ Query vector generation might fail")
    print("‚Ä¢ Ensures search always returns something")


def explain_result_processing():
    """Explain how results are processed and formatted."""
    
    print("\nüìä RESULT PROCESSING")
    print("-" * 25)
    
    print("üîÑ PYTHON PROCESSING:")
    print("‚Ä¢ Loops through pipeline results")
    print("‚Ä¢ Extracts document info from arrays: doc.get('document', [{}])[0]")
    print("‚Ä¢ Extracts knowledge info: doc.get('knowledge', [{}])[0]")
    print("‚Ä¢ Creates clean dictionary for each result")
    print("‚Ä¢ Handles missing data gracefully (defaults to 'Unknown')")
    
    print("\nüìù FINAL RESULT FORMAT:")
    result_example = '''
    {
        "_id": "68ad8bc9d0540da9d9072fd9",
        "chunk_text": "EDUCATION B a c h e l o r...",
        "chunk_index": 5,
        "filename": "Jimson_Ratnam_FullStackDeveloper_2years.pdf",
        "filepath": "s3://bucket/path/file.pdf",
        "summary": "Document about Jimson's background...",
        "keywords": ["education", "technology", "developer"],
        "topic": "Professional Resume",
        "score": 0.6375
    }
    '''
    print(result_example)


def explain_llm_integration():
    """Explain how results are used for LLM answer generation."""
    
    print("\nü§ñ LLM ANSWER GENERATION")
    print("-" * 30)
    
    print("üîó CONTEXT BUILDING:")
    print("‚Ä¢ Takes top 3 results from vector search")
    print("‚Ä¢ Combines: document filename + summary + chunk content")
    print("‚Ä¢ Creates rich context for LLM")
    print("‚Ä¢ Limits chunk content to 300 characters to fit in prompt")
    
    context_example = '''
    Document: Jimson_Ratnam_FullStackDeveloper_2years.pdf
    Summary: Document about Jimson's professional background...
    Content: EDUCATION B a c h e l o r o f T e c h n o l o g y...
    
    Document: Jimson_Ratnam_FullStackDeveloper_2years.pdf  
    Summary: Document about Jimson's professional background...
    Content: Spring Security. ‚Ä¢ Seeking to leverage full s...
    '''
    
    print("üìù CONTEXT EXAMPLE:")
    print(context_example)
    
    print("üéØ LLM PROMPT STRUCTURE:")
    print("‚Ä¢ System message: 'You are an expert assistant...'")
    print("‚Ä¢ Context: Combined document information")
    print("‚Ä¢ Question: Original user query")
    print("‚Ä¢ Instruction: 'Answer in detail based on provided context'")
    
    print("\n‚ö° LLM CALL:")
    print("‚Ä¢ Model: gpt-4o (from your deployment)")
    print("‚Ä¢ Temperature: 0.2 (focused, deterministic answers)")
    print("‚Ä¢ Max tokens: 512 (detailed but concise)")
    print("‚Ä¢ Timeout: 30 seconds")


def explain_complete_flow():
    """Show the complete data flow."""
    
    print("\nüåä COMPLETE DATA FLOW")
    print("-" * 25)
    
    flow = '''
    USER QUERY: "What is the education background?"
         ‚Üì
    EMBEDDING: [1536 float values representing semantic meaning]
         ‚Üì
    VECTOR SEARCH: Compare with 24 chunk embeddings
         ‚Üì
    TOP MATCHES: 3 most similar chunks (scores: 0.6375, 0.5974, 0.5970)
         ‚Üì
    ENRICH DATA: Add document info + knowledge objects
         ‚Üì
    FORMAT RESULTS: Clean, structured data with scores
         ‚Üì
    BUILD CONTEXT: Combine top results for LLM
         ‚Üì
    LLM ANSWER: "Based on the provided context, Jimson Ratnam's education..."
         ‚Üì
    RESPONSE: JSON with answer + detailed results + metadata
    '''
    
    print(flow)


if __name__ == "__main__":
    explain_complete_pipeline()
    explain_stage_1_vector_search()
    explain_stage_2_add_fields()
    explain_stage_3_documents_lookup()
    explain_stage_4_knowledge_lookup()
    explain_stage_5_project()
    explain_error_handling()
    explain_result_processing()
    explain_llm_integration()
    explain_complete_flow()
