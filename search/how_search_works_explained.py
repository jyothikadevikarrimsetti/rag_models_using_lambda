#!/usr/bin/env python3
"""
Comprehensive explanation of how the search pipeline works with the new data model.
This script explains the search flow, data transformations, and result processing.
"""

def explain_search_pipeline():
    """Explain the complete search pipeline workflow."""
    
    print("ðŸ” HOW SEARCH WORKS IN THE NEW DATA MODEL")
    print("=" * 60)
    
    print("\nðŸŽ¯ OVERVIEW")
    print("-" * 30)
    print("The search pipeline uses MongoDB Atlas Vector Search to find relevant")
    print("document chunks based on semantic similarity, then enriches results")
    print("with metadata from related collections.")
    
    print("\nðŸ“Š DATA MODEL STRUCTURE")
    print("-" * 30)
    print("Collections:")
    print("  ðŸ“ chunks          - Text chunks with embeddings")
    print("  ðŸ“ knowledge_objects - Document metadata & summaries")
    print("  ðŸ“ modules         - Module organization & tags")
    print("")
    print("Relationships:")
    print("  chunks.document_id â†’ modules._id")
    print("  knowledge_objects.module_id â†’ modules._id")
    
    print("\nðŸš€ SEARCH FLOW STEP-BY-STEP")
    print("-" * 40)
    
    print("\n1ï¸âƒ£  QUERY PROCESSING")
    print("   Input: User query text (e.g., 'How to configure AWS VPC?')")
    print("   Process:")
    print("   â”Œâ”€ Truncate to 8000 tokens (text-embedding-3-small limit)")
    print("   â”œâ”€ Send to Azure OpenAI embedding API")
    print("   â””â”€ Receive 1536-dimensional vector")
    print("   Output: [0.1, -0.3, 0.8, ..., 0.2] (1536 numbers)")
    
    print("\n2ï¸âƒ£  VECTOR SEARCH")
    print("   Input: Query embedding vector")
    print("   Process:")
    print("   â”Œâ”€ MongoDB Atlas vector search on 'chunks' collection")
    print("   â”œâ”€ Index: 'vector_index' on field 'embedding'")
    print("   â”œâ”€ Similarity: Cosine similarity")
    print("   â”œâ”€ Candidates: 100 (for accuracy)")
    print("   â””â”€ Limit: top_k results (default 3)")
    print("   Output: Ranked chunks with similarity scores")
    
    print("\n3ï¸âƒ£  DATA ENRICHMENT")
    print("   Input: Chunk results with scores")
    print("   Process:")
    print("   â”Œâ”€ Convert chunk.document_id (string) â†’ ObjectId")
    print("   â”œâ”€ JOIN knowledge_objects ON module_id")
    print("   â”œâ”€ JOIN modules ON _id")
    print("   â””â”€ PROJECT relevant fields")
    print("   Output: Enriched results with metadata")
    
    print("\n4ï¸âƒ£  RESULT FORMATTING")
    print("   Input: Raw MongoDB aggregation results")
    print("   Process:")
    print("   â”Œâ”€ Extract fields from nested objects")
    print("   â”œâ”€ Handle missing/optional fields")
    print("   â”œâ”€ Format for consistent structure")
    print("   â””â”€ Include similarity scores")
    print("   Output: Standardized result objects")
    
    print("\n5ï¸âƒ£  CONTEXT PREPARATION")
    print("   Input: Top 3 search results")
    print("   Process:")
    print("   â”Œâ”€ Extract key information per result:")
    print("   â”œâ”€   â€¢ Document title/filename")
    print("   â”œâ”€   â€¢ Summary")
    print("   â”œâ”€   â€¢ Chunk content (first 300 chars)")
    print("   â””â”€ Combine into LLM context")
    print("   Output: Formatted context string")
    
    print("\n6ï¸âƒ£  LLM ANSWER GENERATION")
    print("   Input: Context + user query")
    print("   Process:")
    print("   â”Œâ”€ Create prompt with context and question")
    print("   â”œâ”€ Send to Azure OpenAI (GPT model)")
    print("   â”œâ”€ Temperature: 0.2 (focused answers)")
    print("   â””â”€ Max tokens: 512")
    print("   Output: Natural language answer")
    
    print("\nðŸ”§ AGGREGATION PIPELINE DETAILS")
    print("-" * 40)
    
    pipeline_stages = [
        {
            "stage": "1. $vectorSearch",
            "purpose": "Find similar chunks",
            "details": [
                "â€¢ queryVector: User's embedded query",
                "â€¢ path: 'embedding' (direct field)",
                "â€¢ index: 'vector_index'",
                "â€¢ numCandidates: 100",
                "â€¢ limit: top_k"
            ]
        },
        {
            "stage": "2. $addFields",
            "purpose": "Convert IDs for joins",
            "details": [
                "â€¢ document_object_id = ObjectId(document_id)",
                "â€¢ Enables proper MongoDB joins"
            ]
        },
        {
            "stage": "3. $lookup (knowledge_objects)",
            "purpose": "Get document metadata",
            "details": [
                "â€¢ from: 'knowledge_objects'",
                "â€¢ localField: 'document_object_id'",
                "â€¢ foreignField: 'module_id'",
                "â€¢ as: 'knowledge'"
            ]
        },
        {
            "stage": "4. $lookup (modules)",
            "purpose": "Get module information",
            "details": [
                "â€¢ from: 'modules'",
                "â€¢ localField: 'document_object_id'", 
                "â€¢ foreignField: '_id'",
                "â€¢ as: 'module'"
            ]
        },
        {
            "stage": "5. $project",
            "purpose": "Select relevant fields",
            "details": [
                "â€¢ Chunk: _id, text, positions",
                "â€¢ Knowledge: title, summary, metadata",
                "â€¢ Module: id, tags",
                "â€¢ Score: vectorSearchScore"
            ]
        }
    ]
    
    for stage_info in pipeline_stages:
        print(f"\n{stage_info['stage']}")
        print(f"Purpose: {stage_info['purpose']}")
        for detail in stage_info['details']:
            print(f"   {detail}")
    
    print("\nðŸ“‹ RESULT STRUCTURE")
    print("-" * 25)
    
    sample_result = {
        "_id": "ObjectId('...')",
        "chunk_text": "AWS VPC configuration requires...",
        "chunk_id": 1,
        "chunk_start": 0,
        "chunk_end": 300,
        "filename": "aws-vpc-guide",
        "filepath": "s3://bucket/docs/aws-vpc-guide.pdf",
        "summary": "Comprehensive AWS VPC setup guide",
        "keywords": "aws, vpc, networking, terraform",
        "content": "This document explains...",
        "intent_category": "infrastructure",
        "is_terraform": True,
        "module_id": "mod_aws_vpc",
        "module_tags": ["aws", "infrastructure", "terraform"],
        "score": 0.87
    }
    
    print("Sample search result:")
    for key, value in sample_result.items():
        if isinstance(value, str) and len(value) > 50:
            value = value[:47] + "..."
        print(f"   {key}: {value}")
    
    print("\nâš¡ PERFORMANCE OPTIMIZATIONS")
    print("-" * 35)
    
    optimizations = [
        "ðŸ”¸ Direct embedding storage (no nested arrays)",
        "ðŸ”¸ Vector index on 'embedding' field",
        "ðŸ”¸ Efficient ObjectId conversion",
        "ðŸ”¸ Projection limits returned fields",
        "ðŸ”¸ Connection pooling for MongoDB",
        "ðŸ”¸ Concurrent embedding generation",
        "ðŸ”¸ Timeout handling for API calls",
        "ðŸ”¸ Fallback to text search if vector fails"
    ]
    
    for opt in optimizations:
        print(f"   {opt}")
    
    print("\nðŸ”„ ERROR HANDLING & FALLBACKS")
    print("-" * 35)
    
    error_scenarios = [
        {
            "scenario": "Vector search fails",
            "action": "Fall back to regex text search",
            "impact": "Returns results but with default scores"
        },
        {
            "scenario": "No chunks found",
            "action": "Fall back to legacy search",
            "impact": "Uses old data structure if available"
        },
        {
            "scenario": "OpenAI API timeout",
            "action": "Return embedding/completion error",
            "impact": "User gets error message"
        },
        {
            "scenario": "Missing metadata",
            "action": "Use default values",
            "impact": "Results may have 'Unknown' fields"
        }
    ]
    
    for scenario in error_scenarios:
        print(f"\n   Scenario: {scenario['scenario']}")
        print(f"   Action:   {scenario['action']}")
        print(f"   Impact:   {scenario['impact']}")
    
    print("\nðŸŽ¯ SCORING & RANKING")
    print("-" * 25)
    
    print("Vector similarity scores:")
    print("   â€¢ Range: 0.0 to 1.0")
    print("   â€¢ 1.0 = Perfect match")
    print("   â€¢ 0.8+ = Highly relevant")
    print("   â€¢ 0.6+ = Moderately relevant")
    print("   â€¢ 0.4+ = Somewhat relevant")
    print("   â€¢ <0.4 = Low relevance")
    print("")
    print("Scoring factors:")
    print("   â€¢ Semantic similarity (primary)")
    print("   â€¢ Keyword overlap")
    print("   â€¢ Context understanding")
    print("   â€¢ Document structure")
    
    print("\nðŸ“ˆ SEARCH QUALITY FACTORS")
    print("-" * 30)
    
    quality_factors = [
        "âœ… Embedding model quality (text-embedding-3-small)",
        "âœ… Chunk size optimization (300 chars default)",
        "âœ… Document preprocessing quality",
        "âœ… Metadata completeness",
        "âœ… Vector index configuration",
        "âœ… Query preprocessing",
        "âœ… Result ranking accuracy",
        "âœ… Context window utilization"
    ]
    
    for factor in quality_factors:
        print(f"   {factor}")
    
    print("\n" + "=" * 60)
    print("ðŸŽ‰ SEARCH PIPELINE EXPLANATION COMPLETE!")
    print("\nKey Takeaways:")
    print("â€¢ Uses semantic search with 1536-dim embeddings")
    print("â€¢ Joins 3 collections for comprehensive results")
    print("â€¢ Provides relevance scores for result ranking")
    print("â€¢ Includes robust error handling and fallbacks")
    print("â€¢ Optimized for performance and accuracy")

if __name__ == "__main__":
    explain_search_pipeline()
